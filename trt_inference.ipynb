{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRGAN_PSNR = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/03/2022-11:13:03] [TRT] [E] 1: [defaultAllocator.cpp::allocate::20] Error Code 1: Cuda Runtime (out of memory)\n",
      "[08/03/2022-11:13:03] [TRT] [E] 2: [executionContext.cpp::ExecutionContext::213] Error Code 2: OutOfMemory (no further information)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pylab as plt\n",
    "import json\n",
    "import torch\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "import tensorrt as trt\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.ERROR)\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, '')\n",
    "import common\n",
    "import onnxruntime\n",
    "onnxruntime.disable_telemetry_events()\n",
    "from dalle import TextTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = cuda.mem_alloc(host_mem.nbytes)\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    if isinstance(tensor, numpy.ndarray):\n",
    "        return tensor\n",
    "    else:\n",
    "        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def get_engine_info(engine0):\n",
    "    for b in engine0:\n",
    "        print(b, trt.nptype(engine0.get_binding_dtype(b)), trt.volume(engine0.get_binding_shape(b)) * engine0.max_batch_size)\n",
    "\n",
    "with open('models/vocab.json', 'r', encoding='utf8') as f:\n",
    "    vocab = json.load(f)\n",
    "with open('models/merges.txt', 'r', encoding='utf8') as f:\n",
    "    merges = f.read().split(\"\\n\")[1:-1]\n",
    "\n",
    "ort_session0 = onnxruntime.InferenceSession(\n",
    "    'onnx/encoder0/encoder0.onnx', \n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "ort_session1 = onnxruntime.InferenceSession(\n",
    "    'onnx/encoder1/encoder1.onnx', \n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "ort_session2 = onnxruntime.InferenceSession(\n",
    "    'engines/vqgan.onnx', \n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "\n",
    "tokenizer = TextTokenizer(vocab, merges)\n",
    "runtime = trt.Runtime(TRT_LOGGER)\n",
    "stream = cuda.Stream()\n",
    "\n",
    "with open(\"engines/decoder0.trt\", mode=\"rb\") as f:\n",
    "    engine0 = runtime.deserialize_cuda_engine(f.read())\n",
    "    context0 = engine0.create_execution_context()\n",
    "with open(\"engines/decoder1.trt\", mode=\"rb\") as f:\n",
    "    engine1 = runtime.deserialize_cuda_engine(f.read())\n",
    "    context1 = engine1.create_execution_context()\n",
    "with open(\"engines/decoder2.trt32\", mode=\"rb\") as f:\n",
    "    engine2 = runtime.deserialize_cuda_engine(f.read())\n",
    "    context2 = engine2.create_execution_context()\n",
    "with open(\"engines/srgan_psnr.trt\" if SRGAN_PSNR else \"onnx/srgan.trt\", mode=\"rb\") as f:\n",
    "    engine3 = runtime.deserialize_cuda_engine(f.read())\n",
    "    context3 = engine3.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = 'cat head with sunglasses'\n",
    "TEMPERATURE = 1.0\n",
    "TOPK = 256\n",
    "SFACTOR = 16\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEXT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/airplaneless/source/pythonstuff/dalle-trt-inference/trt_inference.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/airplaneless/source/pythonstuff/dalle-trt-inference/trt_inference.ipynb#ch0000003?line=0'>1</a>\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(TEXT, is_verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[:\u001b[39m64\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/airplaneless/source/pythonstuff/dalle-trt-inference/trt_inference.ipynb#ch0000003?line=1'>2</a>\u001b[0m text_tokens \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mones((\u001b[39m2\u001b[39m, \u001b[39m64\u001b[39m), dtype\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mint32)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/airplaneless/source/pythonstuff/dalle-trt-inference/trt_inference.ipynb#ch0000003?line=2'>3</a>\u001b[0m text_tokens[\u001b[39m0\u001b[39m, :\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m [tokens[\u001b[39m0\u001b[39m], tokens[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEXT' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(TEXT, is_verbose=False)[:64]\n",
    "text_tokens = numpy.ones((2, 64), dtype=numpy.int32)\n",
    "text_tokens[0, :2] = [tokens[0], tokens[-1]]\n",
    "text_tokens[1, :len(tokens)] = tokens\n",
    "text_tokens = torch.tensor(\n",
    "    text_tokens, \n",
    "    dtype=torch.long, \n",
    ")\n",
    "\n",
    "ort_inputs = {ort_session0.get_inputs()[0].name: to_numpy(text_tokens)}\n",
    "ort_outs = ort_session0.run(None, ort_inputs)\n",
    "ort_inputs = {ort_session1.get_inputs()[0].name: to_numpy(ort_outs[0]), ort_session1.get_inputs()[1].name: to_numpy(text_tokens)}\n",
    "ort_outs = ort_session1.run(None, ort_inputs)\n",
    "encoder_state = torch.from_numpy(ort_outs[0])\n",
    "print(encoder_state.shape)\n",
    "\n",
    "expanded_indices = [0] * 1 + [1] * 1\n",
    "text_tokens = text_tokens[expanded_indices]\n",
    "encoder_state = encoder_state[expanded_indices]\n",
    "attention_mask = text_tokens.not_equal(1).long()\n",
    "attention_state = torch.zeros(size=(24, 1 * 4, 256, 2048))\n",
    "image_tokens = torch.full((256 + 1, 1), 16415, dtype=torch.long)\n",
    "torch.manual_seed(SEED)\n",
    "token_indices = torch.arange(256)\n",
    "settings = torch.tensor([TEMPERATURE, TOPK, SFACTOR])\n",
    "# init cuda\n",
    "stream = cuda.Stream()\n",
    "tAM = HostDeviceMem(cuda.pagelocked_empty(128, numpy.int32))\n",
    "tES = HostDeviceMem(cuda.pagelocked_empty(262144, numpy.float32))\n",
    "tIT = HostDeviceMem(cuda.pagelocked_empty(1, numpy.int32))\n",
    "tTI = HostDeviceMem(cuda.pagelocked_empty(1, numpy.int32))\n",
    "tDS = HostDeviceMem(cuda.pagelocked_empty(4096, numpy.float32))\n",
    "tOut = HostDeviceMem(cuda.pagelocked_empty(32832, numpy.float32))\n",
    "tAS0 = HostDeviceMem(cuda.pagelocked_empty(16777216, numpy.float32))\n",
    "tAS1 = HostDeviceMem(cuda.pagelocked_empty(16777216, numpy.float32))\n",
    "tAS2 = HostDeviceMem(cuda.pagelocked_empty(16777216, numpy.float32))\n",
    "\n",
    "numpy.copyto(tAM.host, to_numpy(attention_mask).ravel())\n",
    "numpy.copyto(tES.host, to_numpy(encoder_state).ravel())\n",
    "numpy.copyto(tAS0.host, to_numpy(attention_state[:8]).ravel())\n",
    "numpy.copyto(tAS1.host, to_numpy(attention_state[8:16]).ravel())\n",
    "numpy.copyto(tAS2.host, to_numpy(attention_state[16:]).ravel())\n",
    "cuda.memcpy_htod_async(tAM.device, tAM.host, stream)\n",
    "cuda.memcpy_htod_async(tES.device, tES.host, stream)\n",
    "cuda.memcpy_htod_async(tAS0.device, tAS0.host, stream)\n",
    "cuda.memcpy_htod_async(tAS1.device, tAS1.host, stream)\n",
    "cuda.memcpy_htod_async(tAS2.device, tAS2.host, stream)\n",
    "for i in tqdm(range(256)):\n",
    "    numpy.copyto(tIT.host, to_numpy(image_tokens[i]).ravel())\n",
    "    numpy.copyto(tTI.host, to_numpy(token_indices[[i]]).ravel())\n",
    "    cuda.memcpy_htod_async(tIT.device, tIT.host, stream)\n",
    "    cuda.memcpy_htod_async(tTI.device, tTI.host, stream)\n",
    "    queue = [tAM, tES, tAS0, tIT, tTI, tDS, tAS0]\n",
    "    context0.execute_async_v2(bindings=[v.device for v in queue], stream_handle=stream.handle)\n",
    "    stream.synchronize()\n",
    "    queue = [tAM, tES, tDS, tAS1, tTI, tDS, tAS1]\n",
    "    context1.execute_async_v2(bindings=[v.device for v in queue], stream_handle=stream.handle)\n",
    "    stream.synchronize()\n",
    "    queue = [tAM, tES, tDS, tAS2, tTI, tAS2, tOut]\n",
    "    context2.execute_async_v2(bindings=[v.device for v in queue], stream_handle=stream.handle)\n",
    "    cuda.memcpy_dtoh_async(tOut.host, tOut.device, stream)\n",
    "    stream.synchronize()\n",
    "    logits = torch.from_numpy(tOut.host).reshape(2, 1, 16416)\n",
    "    logits = logits[:, -1, : 2 ** 14]\n",
    "    temperature = settings[[0]]\n",
    "    top_k = settings[[1]].to(torch.long)\n",
    "    supercondition_factor = settings[[2]]\n",
    "    logits = (\n",
    "        logits[:1] * (1 - supercondition_factor) + \n",
    "        logits[1:] * supercondition_factor\n",
    "    )\n",
    "    logits_sorted, _ = logits.sort(descending=True)\n",
    "    is_kept = logits >= logits_sorted[:, top_k - 1]\n",
    "    logits -= logits_sorted[:, [0]]\n",
    "    logits /= temperature\n",
    "    logits.exp_()\n",
    "    logits *= is_kept.to(torch.float32)\n",
    "    image_tokens[i + 1] = torch.multinomial(logits, 1)[:, 0]\n",
    "\n",
    "ort_inputs = {ort_session2.get_inputs()[0].name: to_numpy(image_tokens[1:].T)}\n",
    "images = ort_session2.run(None, ort_inputs)[0]\n",
    "\n",
    "image_outputs = []\n",
    "for i in range(images.shape[0]):\n",
    "    # inputs, outputs, bindings = common.allocate_buffers(engine3)\n",
    "    # numpy.copyto(inputs[0].host, numpy.moveaxis(images[i][None], -1, 1).ravel() / 255.)\n",
    "    # img = common.do_inference_v2(context3, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)[0]\n",
    "    img = numpy.moveaxis(images[i][None], -1, 1).ravel() / 255.\n",
    "    img = numpy.moveaxis(img.reshape(3,1024,1024), 0, -1).clip(0, 1)\n",
    "    image_outputs.append(img)\n",
    "\n",
    "for i in range(len(image_outputs)):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(image_outputs[i])\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.fromarray((image_outputs[0] * 255.).astype(numpy.uint8)).save('output2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
